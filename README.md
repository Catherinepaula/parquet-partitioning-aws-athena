# AWS Data Lake Project â€“ Parquet Partitioning with Athena

## ğŸ“Œ VisÃ£o Geral

Este projeto demonstra a implementaÃ§Ã£o de um fluxo completo de persistÃªncia e consulta de dados em ambiente cloud utilizando serviÃ§os da AWS e tÃ©cnicas de otimizaÃ§Ã£o baseadas em armazenamento colunar.

O conjunto de dados foi processado com Pandas e persistido em formato Parquet, com compressÃ£o Snappy e particionamento por data, visando otimizaÃ§Ã£o de performance em consultas analÃ­ticas.

---

## ğŸ›  Tecnologias Utilizadas

- Python
- Pandas
- PyArrow
- Amazon S3
- Amazon Athena
- SQL

---

## âš™ï¸ Etapas do Projeto

1. IngestÃ£o e leitura do dataset em DataFrame Pandas  
2. PadronizaÃ§Ã£o dos nomes das colunas  
3. ConversÃ£o de tipos de dados  
4. CriaÃ§Ã£o da coluna `reference_date` para particionamento  
5. PersistÃªncia dos dados em:
   - CSV (formato orientado a linha)
   - Parquet (formato orientado a coluna, compressÃ£o Snappy)
6. Particionamento dos arquivos por `reference_date`
7. Upload do dataset particionado para Amazon S3
8. CriaÃ§Ã£o de tabela externa no Amazon Athena
9. AtualizaÃ§Ã£o das partiÃ§Ãµes com `MSCK REPAIR TABLE`
10. ExecuÃ§Ã£o de consultas SQL para validaÃ§Ã£o

---

## ğŸ§  Conceitos Aplicados

- Armazenamento colunar (Parquet)
- CompressÃ£o de dados
- EstratÃ©gias de particionamento
- Arquitetura de Data Lake
- IntegraÃ§Ã£o entre Amazon S3 e Amazon Athena
- Consultas serverless
- OtimizaÃ§Ã£o de performance em workloads analÃ­ticos

---

## ğŸš€ Resultados

A integraÃ§Ã£o entre Amazon S3 e Amazon Athena foi validada com sucesso, com reconhecimento correto das partiÃ§Ãµes e execuÃ§Ã£o eficiente das consultas SQL.

O particionamento por `reference_date` permitiu que as consultas fossem realizadas de maneira mais otimizada, reduzindo o volume de dados escaneados.

---

## ğŸ“Š Arquitetura Simplificada

Data â†’ Pandas â†’ Parquet (Particionado) â†’ Amazon S3 â†’ Amazon Athena â†’ SQL
